20-04-24 15:23[root]INFO | Training on env: walker
20-04-24 15:23[root]INFO | Agent Type: dgt
20-04-24 15:23[root]INFO | Log Directory: ./logs/walker_cpu/dgt/128leaves_17-23-15-20-Apr_Final/
20-04-24 15:23[root]INFO | GPU?: False
20-04-24 15:23[root]INFO | Num Leaves: 128
20-04-24 15:23[root]INFO | DGT Policy
20-04-24 15:23[root]INFO | Value Net Size: [256, 256]
20-04-24 15:23[root]INFO | [START]======> Training Started for Seed [23]: 2024-04-20 15:23:24
20-04-24 15:23[root]INFO | actor: DGTActor(
  (features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (drnet): DGT(
    (_and_act_fn): Softmax(dim=-1)
    (_predicate_l): Sequential(
      (0): Linear(in_features=24, out_features=127, bias=True)
    )
    (_and_l): XLinear(
      (_l): Linear(in_features=127, out_features=128, bias=False)
    )
    (_or_l): XLinear(
      (_l): Linear(in_features=128, out_features=4, bias=False)
    )
  )
)
20-04-24 15:23[icct.rl_helpers.eval_callback]INFO | (Eval ) num_timesteps=1500, Reward=-92.09 +/- 0.12, EpLength: 114.00 +/- 1.10
20-04-24 15:23[icct.rl_helpers.eval_callback]INFO | (Save ) Storing Best Model: New best mean reward!
20-04-24 15:23[icct.rl_helpers.eval_callback]INFO | (Train) Episode: 0, Avg Reward: -83.35245116192823, Best: -83.35245116192823
20-04-24 15:23[icct.rl_helpers.eval_callback]INFO | (Eval ) num_timesteps=3000, Reward=-92.20 +/- 0.05, EpLength: 112.20 +/- 1.47
20-04-24 15:23[icct.rl_helpers.eval_callback]INFO | (Eval ) num_timesteps=4500, Reward=-92.21 +/- 0.14, EpLength: 112.00 +/- 1.10
20-04-24 15:23[icct.rl_helpers.eval_callback]INFO | (Eval ) num_timesteps=6000, Reward=-92.16 +/- 0.09, EpLength: 112.00 +/- 1.10
20-04-24 15:23[icct.rl_helpers.eval_callback]INFO | (Train) Episode: 10, Avg Reward: -102.01928043491925, Best: -83.35245116192823
20-04-24 15:23[icct.rl_helpers.eval_callback]INFO | (Eval ) num_timesteps=7500, Reward=-92.21 +/- 0.08, EpLength: 112.20 +/- 1.33
20-04-24 15:23[icct.rl_helpers.eval_callback]INFO | (Eval ) num_timesteps=9000, Reward=-92.19 +/- 0.07, EpLength: 112.40 +/- 1.74
20-04-24 15:23[icct.rl_helpers.eval_callback]INFO | (Train) Episode: 20, Avg Reward: -103.73881640629514, Best: -83.35245116192823
20-04-24 15:23[icct.rl_helpers.eval_callback]INFO | (Eval ) num_timesteps=10500, Reward=-99.45 +/- 0.05, EpLength: 60.40 +/- 0.49
20-04-24 15:25[icct.rl_helpers.eval_callback]INFO | (Eval ) num_timesteps=12000, Reward=-109.66 +/- 0.59, EpLength: 67.80 +/- 2.48
20-04-24 15:25[icct.rl_helpers.eval_callback]INFO | (Train) Episode: 30, Avg Reward: -109.97485582991891, Best: -83.35245116192823
20-04-24 15:26[icct.rl_helpers.eval_callback]INFO | (Eval ) num_timesteps=13500, Reward=-122.27 +/- 0.85, EpLength: 122.60 +/- 16.56
20-04-24 15:27[icct.rl_helpers.eval_callback]INFO | (Eval ) num_timesteps=15000, Reward=-105.82 +/- 0.46, EpLength: 72.60 +/- 1.36
20-04-24 15:28[icct.rl_helpers.eval_callback]INFO | (Train) Episode: 40, Avg Reward: -111.78799174792046, Best: -83.35245116192823
20-04-24 15:28[icct.rl_helpers.eval_callback]INFO | (Eval ) num_timesteps=16500, Reward=-104.67 +/- 0.12, EpLength: 65.60 +/- 0.49
20-04-24 15:29[icct.rl_helpers.eval_callback]INFO | (Eval ) num_timesteps=18000, Reward=-106.10 +/- 0.35, EpLength: 81.40 +/- 1.36
